{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating-point sowing and rewards v1.6\n",
    "\n",
    "The environment now largely conforms to the OpenAI env style, with the env written as a class having a standard set of methods (step, reset, render, close).  AFAICT, making this into a proper OpenAI env means setting up a special directory structure such that it can be loaded and run with the `gym.make()` function---but that isn't nearly as readable as a notebook like this.  The following three cells have the environment and agent code.  \n",
    "\n",
    "The original (oldest) agent is very dumb; it merely samples from the space of possible action-weight distributions and sows according to the sampled distribution.  Iteration allows it to keep the best distribution it has discovered.  \n",
    "\n",
    "The newer agent (just below the env code) is slightly smarter.  It takes a biased random walk through weight space, committing to only those steps which produce an increased yield at the end of an episode.  (You can see that, at the moment, it tends toward favoring squash, which in the current reward function provides the highest single-plant reward.)\n",
    "\n",
    "To-do list for the **environment**:\n",
    "\n",
    "* try out official OpenAI file + directory structure\n",
    "* determine the correct gym `space` for the observation space (an *n* x 3 array where the value of *n* isn't fixed until the end of the episode)\n",
    "* treat plants as enumeration class (maybe? need to make sure attributes like age still available)\n",
    "* elaborate reward functions and make them more biologically plausible\n",
    "* * in particular, write something extremely plausible for maize-only planting? (for our beyond-the-row paper)\n",
    "* update the `render()` code:\n",
    "* * have one output option be a series of 6 snapshots illustrating an agent's learning\n",
    "* * another output option for a movie\n",
    "\n",
    "\n",
    "To-do list for the **agent**:\n",
    "\n",
    "* write a variety of learning algorithms (to have a bake-off) and wrap them in functions.  most obviously this would include RL algorithms, but shouldn't overlook e.g. quality diversity\n",
    "* * look at what sklearn has to offer\n",
    "* currently the only thing the agent can observe is the .field property, which is the location and species of each plant.  add a separate .state property of the environment with additional features to observe?  for example, the ages of each plant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from enum import Enum\n",
    "\n",
    "class Plant:\n",
    "    def __init__(self, species, maturity=110):\n",
    "        self.species = species\n",
    "        self.maturity = maturity\n",
    "        self.age = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"{}\".format(self.species)\n",
    "    \n",
    "\n",
    "class Field(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, size=5, sow_limit=200, season=120, calendar=0):\n",
    "        # parameters for overall field character\n",
    "        self.size = size\n",
    "        self.sow_limit = sow_limit\n",
    "        self.season = season\n",
    "        self.calendar = calendar\n",
    "        \n",
    "        # constants for computing end-of-season reward---distances represent meters\n",
    "        self.crowding_dist = .02\n",
    "        self.maize_maize_dist = .1\n",
    "        self.bean_support_dist = .1\n",
    "        self.crowding_penalty = .1\n",
    "        self.maize_maize_penalty = .9\n",
    "        self.bean_support_bonus = .6\n",
    "        \n",
    "        # OpenAI action and observation space specifications\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # self.observation_space = spaces.???\n",
    "        \n",
    "        # field is initialized by calling reset()\n",
    "        self.field = None\n",
    "        \n",
    "    def step(self, action):\n",
    "        # sow plants (or wait) depending on actions chosen\n",
    "        # action is an array of n choices; value of n specified in agent code sow_limit\n",
    "        # could be cleaned up with plants as an enumeration?\n",
    "        \n",
    "        for choice in action:\n",
    "            if choice == 0:\n",
    "                self.field = np.append(self.field, [[self.size * np.random.random(), \n",
    "                                             self.size * np.random.random(), \n",
    "                                             Plant('Maize')]], axis=0)\n",
    "            elif choice == 1:\n",
    "                self.field = np.append(self.field, [[self.size * np.random.random(), \n",
    "                                             self.size * np.random.random(), \n",
    "                                             Plant('Bean')]], axis=0)\n",
    "            elif choice == 2:\n",
    "                self.field = np.append(self.field, [[self.size * np.random.random(), \n",
    "                                             self.size * np.random.random(), \n",
    "                                             Plant('Squash')]], axis=0)\n",
    "            elif choice == 3:\n",
    "                self.field = np.append(self.field, [[0.0,0.0,\"Wait\"]], axis=0)\n",
    "                \n",
    "            \n",
    "        \n",
    "        # increment timekeeping\n",
    "        self.calendar +=1\n",
    "        for plant in self.field:\n",
    "            if type(plant[2]) != str:\n",
    "                plant[2].age += 1\n",
    "            \n",
    "        done = self.calendar == self.season\n",
    "            \n",
    "        if not done:\n",
    "            reward = 0\n",
    "        else:\n",
    "#             print(\"DONE\")\n",
    "            reward = self.get_reward()\n",
    "#             print(\"DONE2\")\n",
    "            \n",
    "        return self.field, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # field is initialized with one random corn plant in order to make sowing (by np.append) work\n",
    "        self.field = np.array([[self.size * np.random.random(), \n",
    "                                self.size * np.random.random(), \n",
    "                                Plant('Maize')]])\n",
    "        # timekeeping is reset\n",
    "        self.calendar = 0\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        # initialize plant type arrays so that pyplot won't break if any is empty\n",
    "        maize = np.array([[None, None]])\n",
    "        bean = np.array([[None, None]])\n",
    "        squash = np.array([[None, None]])\n",
    "        maize_imm = np.array([[None, None]])\n",
    "        bean_imm = np.array([[None, None]])\n",
    "        squash_imm = np.array([[None, None]])\n",
    "        \n",
    "        # replace initial arrays with coordinates for each plant type\n",
    "        maize = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Maize' and row[2].age >= row[2].maturity])\n",
    "        bean = np.array([row for row in self.field \n",
    "                            if row[2].__repr__() == 'Bean' and row[2].age >= row[2].maturity])\n",
    "        squash = np.array([row for row in self.field \n",
    "                              if row[2].__repr__() == 'Squash' and row[2].age >= row[2].maturity])\n",
    "        maize_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Maize' and row[2].age < row[2].maturity])\n",
    "        bean_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Bean' and row[2].age < row[2].maturity])\n",
    "        squash_imm = np.array([row for row in self.field \n",
    "                             if row[2].__repr__() == 'Squash' and row[2].age < row[2].maturity])\n",
    "        \n",
    "        # plot the field---currently breaks if any plant type is absent\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(maize[:,0], maize[:,1], c='green', s=200, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(bean[:,0], bean[:,1], c='brown', s=150, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(squash[:,0], squash[:,1], c='orange', s=400, marker = 'o', alpha=.5, edgecolor='#303030')\n",
    "        plt.scatter(maize_imm[:,0], maize_imm[:,1], c='green', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "        plt.scatter(bean_imm[:,0], bean_imm[:,1], c='brown', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "        plt.scatter(squash_imm[:,0], squash_imm[:,1], c='orange', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Total yield in Calories is {}.\\n---\\n\".format(round(reward, 1)))\n",
    "    \n",
    "    def close(self):\n",
    "        # unneeded right now? AFAICT this is only used to shut down realtime movie visualizations\n",
    "        pass\n",
    "    \n",
    "    def get_reward(self):\n",
    "        # array of plant coordinates for computing distances\n",
    "        xy_array = np.array([[row[0], row[1]] for row in self.field if type(row[2]) != str])\n",
    "        xy_array = xy_array.astype(float)\n",
    "#         print(xy_array)\n",
    "\n",
    "        # distances[m,n] is distance from mth to nth plant in field\n",
    "        distances = np.linalg.norm(xy_array - xy_array[:,None], axis=-1)\n",
    "        \n",
    "        reward = 0\n",
    "        i = 0\n",
    "        count = sum(1 for i in self.field if type(i[2]) == str)\n",
    "#         print(\"length\", len(self.field), \"count\", count, \"distance\", len(distances[0]))\n",
    "        while i < (len(self.field) - (count)):\n",
    "#             print(i)\n",
    "            if type(self.field[i,2]) != str:\n",
    "                if self.field[i,2].age < self.field[i,2].maturity:\n",
    "                    reward += 0\n",
    "                elif self.field[i,2].__repr__() == 'Maize':\n",
    "                    cal = 1\n",
    "                    j = 0\n",
    "                    while j < len(distances[0]):\n",
    "                        if (self.field[j,2].__repr__() == 'Bean' \n",
    "                                and distances[i,j] < self.bean_support_dist):\n",
    "                            cal += self.bean_support_bonus\n",
    "                        if (self.field[j,2].__repr__() == 'Maize' \n",
    "                                and i !=j \n",
    "                                and distances[i,j] < self.maize_maize_dist):\n",
    "                            cal *= self.maize_maize_penalty\n",
    "                        if 0 < distances[i,j] < self.crowding_dist:\n",
    "                            cal *= self.crowding_penalty\n",
    "                        j += 1\n",
    "                    reward += cal\n",
    "                elif self.field[i,2].__repr__() == 'Bean':\n",
    "                    reward += .25\n",
    "                elif self.field[i,2].__repr__() == 'Squash':\n",
    "                    reward += 3\n",
    "            i += 1        \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc74abaaa9e74dfb908175de6f5ae5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# second agent: biased search in weight space\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# global parameters for trials\n",
    "testbed = Field()\n",
    "best_weights = np.random.dirichlet(np.ones(4))\n",
    "best_yield = 0\n",
    "yield_history = []\n",
    "sow_limit = 3\n",
    "num_trials = 100\n",
    "\n",
    "# for storing subplots\n",
    "subplots = []\n",
    "\n",
    "total_fields_generated = 0\n",
    "num_fields_to_generate = 1000\n",
    "step= 1\n",
    "pbar = tqdm(total=num_fields_to_generate)\n",
    "\n",
    "for j in range(num_fields_to_generate):\n",
    "    \n",
    "#     pbar.update(total_fields_generated)\n",
    "\n",
    "    # does a random walk in weight space and keeps any step that improves yield\n",
    "    for i in range(num_trials):\n",
    "\n",
    "        # initialize trial\n",
    "        observation = testbed.reset()\n",
    "    #     print(\"init observation\", observation)\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "        # calculate step vector for random walk\n",
    "        epsilon = np.random.random(1) / 20\n",
    "        delta = epsilon * np.random.permutation([1, 1, -1, -1])\n",
    "\n",
    "        while not done:\n",
    "            count += 1\n",
    "\n",
    "            # ensure step won't take agent outside of weight space\n",
    "            while True in ((best_weights + delta) < 0):\n",
    "                epsilon = np.random.random(1) / 20\n",
    "                delta = epsilon * np.random.permutation([1, 1, -1, -1])\n",
    "            weights = best_weights + delta\n",
    "            action = [np.random.choice(4, p=weights) for i in range(sow_limit)]\n",
    "    #         print(\"action\", action)\n",
    "            observation, reward, done, _ = testbed.step(action)\n",
    "#             print(\"HERE\")\n",
    "            if done:\n",
    "#                 print(\"Done\")\n",
    "                yield_history.append(round(reward))\n",
    "                if reward > best_yield:\n",
    "                    best_yield = reward\n",
    "                    best_weights = weights\n",
    "\n",
    "    #     if i % 20 == 0:\n",
    "    #         print(\"Results after {} trials:\".format(i))\n",
    "    #         subplot = testbed.render()\n",
    "    #         # subplots.append(subplot) # attempting to get a six-in-one figure\n",
    "\n",
    "    \n",
    "    # run a final trial using the best weights discovered\n",
    "    observation = testbed.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        weights = best_weights\n",
    "        action = [np.random.choice(4, p=best_weights) for i in range(sow_limit)]\n",
    "        observation, reward, done, _ = testbed.step(action)\n",
    " \n",
    "    age_col = []\n",
    "    for row in testbed.field:\n",
    "        if type(row[2]) != str:\n",
    "            age_col.append(\"Mature\" if row[2].age >= row[2].maturity else \"Immature\")\n",
    "        else:\n",
    "            age_col.append(\"None\")\n",
    "    \n",
    "\n",
    "    age_col = np.reshape(age_col, (-1, 1))\n",
    "    field_file_obj = np.concatenate((testbed.field, age_col), axis=1)\n",
    "        \n",
    "    np.savetxt(\"../Field Data/field%s.txt\" %j, field_file_obj, fmt=\"%s %s %s %s\")\n",
    "    with open(\"../Field Data/field%s.txt\" %j, \"a+\") as myfile:\n",
    "        myfile.write(\"Yield: \" + str(testbed.get_reward()))\n",
    "        \n",
    "    pbar.update(step)\n",
    "\n",
    "    #print(subplots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d245561f11ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msow_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestbed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f5a6a5480fe7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                              Plant('Squash')]], axis=0)\n\u001b[1;32m     61\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Wait\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4743\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4744\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4745\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# simple agent: takes random sample of weight space to arrive at best weights\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "testbed = Field()\n",
    "best_weights = np.zeros(4)\n",
    "best_yield = 0\n",
    "yield_history = []\n",
    "sow_limit = 10\n",
    "\n",
    "\n",
    "# run the trials where each set of weights produces a yield\n",
    "for i in range(100):\n",
    "    observation = testbed.reset()\n",
    "    done = False\n",
    "    count = 0\n",
    "    while not done:\n",
    "        count += 1\n",
    "        weights = np.random.dirichlet(np.ones(4))\n",
    "        action = [np.random.choice(4, p=weights) for i in range(sow_limit)]\n",
    "        observation, reward, done, _ = testbed.step(action)\n",
    "        \n",
    "        if done:\n",
    "            yield_history.append(round(reward))\n",
    "            if reward > best_yield:\n",
    "                best_yield = reward\n",
    "                best_weights = weights\n",
    "\n",
    "\n",
    "# run a final trial using the best weights discovered\n",
    "observation = testbed.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    weights = best_weights\n",
    "    action = [np.random.choice(4, p=best_weights) for i in range(sow_limit)]\n",
    "    observation, reward, done, _ = testbed.step(action)\n",
    "    \n",
    "\n",
    "# show results\n",
    "#print(yield_history)\n",
    "#print(best_yield)\n",
    "print(best_weights)\n",
    "            \n",
    "testbed.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
